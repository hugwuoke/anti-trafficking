{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import mode\n",
    "import faiss\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from treeple import UnsupervisedRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"Data/Simulated Data/New Simulated Data t=21/simple_gen_missing_data_new.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "vars = df.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable X1: mode value 0.0\n",
      "Variable X2: mode value 0.0\n",
      "Variable X3: mode value 0.0\n",
      "Variable X4: mode value 0.0\n",
      "Variable X5: mode value 0.0\n",
      "Variable X6: mode value 0.0\n",
      "Variable X7: mode value 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_11658/3159576940.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_popular_imp[var].fillna(mode_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Imputation by most common value per variable (aka popularity)\n",
    "output_file = \"Simulated Data/simple_gen_mode_impute_new.csv\"\n",
    "df_popular_imp = df.copy()\n",
    "\n",
    "for var in vars:\n",
    "    mode_value = df_popular_imp[var].mode()[0]\n",
    "    print(f'Variable {var}: mode value {mode_value}')\n",
    "    df_popular_imp[var].fillna(mode_value, inplace=True)\n",
    "\n",
    "df_popular_imp.to_csv(output_file, index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation by Prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable X1: values [0.0, 1.0] probability [0.7999652674407065, 0.20003473255929344]\n",
      "Variable X2: values [0.0, 1.0] probability [0.8984598659177387, 0.10154013408226129]\n",
      "Variable X3: values [0.0, 1.0] probability [0.8230522473360127, 0.17694775266398732]\n",
      "Variable X4: values [0.0, 1.0] probability [0.8911992869180095, 0.1088007130819905]\n",
      "Variable X5: values [0.0, 1.0] probability [0.9105684441777443, 0.08943155582225568]\n",
      "Variable X6: values [0.0, 1.0] probability [0.9068353343745902, 0.09316466562540984]\n",
      "Variable X7: values [0.0, 1.0] probability [0.8930360977605072, 0.10696390223949279]\n"
     ]
    }
   ],
   "source": [
    "# Imputation by probability distribution of existing values\n",
    "output_file = \"Simulated Data/simple_gen_prevalence_impute_new.csv\"\n",
    "df_prob_imp = df.copy()\n",
    "\n",
    "for var in vars:\n",
    "    # Get the distribution of non-missing values\n",
    "    freq_table = df_prob_imp[var].value_counts(dropna=True)\n",
    "    distinct_vals = freq_table.index.to_list()\n",
    "    probabilities = (freq_table / freq_table.sum()).to_list()\n",
    "\n",
    "    print(f'Variable {var}: values {distinct_vals} probability {probabilities}')\n",
    "    \n",
    "    # Identify which rows are missing\n",
    "    missing_mask = df_prob_imp[var].isna()\n",
    "    n_missing = missing_mask.sum()\n",
    "    \n",
    "    # Randomly sample from the distinct values, using the same distribution\n",
    "    random_draws = np.random.choice(distinct_vals, size=n_missing, p=probabilities)\n",
    "    \n",
    "    # Fill in the missing values\n",
    "    df_prob_imp.loc[missing_mask, var] = random_draws\n",
    "\n",
    "df_prob_imp.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Imputation (binary only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Simulated Data/simple_gen_logreg_impute_new.csv\"\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LogisticRegression(),\n",
    "    max_iter=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "df_imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "df_logreg_imputed = pd.DataFrame(df_imputed_array, columns=df.columns)\n",
    "df_logreg_imputed.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Imputation (continuous only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_file = \"Simulated Data/Normal (Non-Categorical)/non_categorical_gen_linreg_imputation.csv\"\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=LinearRegression(),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_imputed_array = imputer.fit_transform(df)\n",
    "\n",
    "df_linreg_imputed = pd.DataFrame(df_imputed_array, columns=df.columns)\n",
    "df_linreg_imputed.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79dbdf0dc14bb8a58b421806aedb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Imputing on full dataset (no subsampling):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final (no subsampling) imputed CSV to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavgk3,5,7_impute_binary.csv\n"
     ]
    }
   ],
   "source": [
    "# ------------- USER PARAMETERS -------------\n",
    "n_subsamples = 0\n",
    "sample_with_replacement = False\n",
    "k_values = [3, 5, 7]\n",
    "binary_cols = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"]\n",
    "\n",
    "# Note how the templates include both {n_subsamples} and {sub_i}\n",
    "partial_output_template = (\n",
    "    \"Data/Simulated Data/New Simulated Data t=21/kNN/{n_subsamples}k3,5/simple_gen_kNNavg{sub_i}k3,5,7_impute_partial.csv\"\n",
    ")\n",
    "final_output_template = (\n",
    "    \"Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavgk3,5,7_impute_binary.csv\"\n",
    ")\n",
    "\n",
    "# We must format the final_output_template with n_subsamples\n",
    "final_output_file = final_output_template.format(n_subsamples=n_subsamples)\n",
    "\n",
    "# ------------- PRECOMPUTE COLUMN MODES FOR TIE-BREAKING -------------\n",
    "col_modes = {}\n",
    "for col in binary_cols:\n",
    "    if df[col].notna().sum() == 0:\n",
    "        col_modes[col] = 0\n",
    "    else:\n",
    "        col_modes[col] = df[col].mode().iloc[0]\n",
    "\n",
    "# ------------- HELPER: FORCE TO BINARY -------------\n",
    "def force_to_binary(imputed_array, df_reference, binary_cols, col_modes):\n",
    "    for col in binary_cols:\n",
    "        col_idx = df_reference.columns.get_loc(col)\n",
    "        col_values = imputed_array[:, col_idx]\n",
    "\n",
    "        mask_lt = col_values < 0.5\n",
    "        mask_gt = col_values > 0.5\n",
    "        mask_eq = np.isclose(col_values, 0.5)\n",
    "\n",
    "        col_values[mask_lt] = 0\n",
    "        col_values[mask_gt] = 1\n",
    "        col_values[mask_eq] = col_modes[col]\n",
    "\n",
    "        imputed_array[:, col_idx] = col_values\n",
    "\n",
    "    return imputed_array\n",
    "\n",
    "# ------------- MAIN LOGIC -------------\n",
    "all_imputations = []  # Collect all imputed arrays for final averaging\n",
    "\n",
    "if n_subsamples == 0:\n",
    "    # NO SUBSAMPLING\n",
    "    imputed_list = []\n",
    "    for k in tqdm(k_values, desc=\"Imputing on full dataset (no subsampling)\"):\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputer.fit(df)\n",
    "        imputed_data = imputer.transform(df)\n",
    "        imputed_list.append(imputed_data)\n",
    "\n",
    "    avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "    avg_imputed_data = force_to_binary(avg_imputed_data, df, binary_cols, col_modes)\n",
    "    df_imputed_final = pd.DataFrame(avg_imputed_data, columns=df.columns)\n",
    "\n",
    "    df_imputed_final.to_csv(final_output_file, index=False)\n",
    "    print(f\"Saved final (no subsampling) imputed CSV to {final_output_file}\")\n",
    "\n",
    "else:\n",
    "    # WITH SUBSAMPLING\n",
    "    for sub_i in tqdm(range(1, n_subsamples + 1), desc=\"Subsamples\"):\n",
    "        subsample = df.groupby('t', group_keys=False).apply(\n",
    "            lambda x: x.sample(frac=1, replace=sample_with_replacement)\n",
    "        )\n",
    "\n",
    "        sub_imputed_list = []\n",
    "\n",
    "        for k in tqdm(k_values, desc=f\"k-values for Subsample {sub_i}\", leave=False):\n",
    "            imputer = KNNImputer(n_neighbors=k)\n",
    "            imputer.fit(subsample)\n",
    "            imputed_data = imputer.transform(df)\n",
    "\n",
    "            sub_imputed_list.append(imputed_data)\n",
    "            all_imputations.append(imputed_data)\n",
    "\n",
    "        # Average across all k-values for this subsample\n",
    "        sub_avg_imputed_data = np.mean(np.stack(sub_imputed_list, axis=0), axis=0)\n",
    "        sub_avg_imputed_data = force_to_binary(sub_avg_imputed_data, df, binary_cols, col_modes)\n",
    "\n",
    "        df_sub_imputed = pd.DataFrame(sub_avg_imputed_data, columns=df.columns)\n",
    "\n",
    "        # Now format the partial_output_template with BOTH sub_i and n_subsamples\n",
    "        partial_output_file = partial_output_template.format(\n",
    "            n_subsamples=n_subsamples,\n",
    "            sub_i=sub_i\n",
    "        )\n",
    "        df_sub_imputed.to_csv(partial_output_file, index=False)\n",
    "\n",
    "        print(f\"Saved partial imputed dataset for subsample {sub_i} -> {partial_output_file}\")\n",
    "\n",
    "    # FINAL AVERAGE ACROSS ALL SUBSAMPLES & K-VALUES\n",
    "    final_avg_imputed_data = np.mean(np.stack(all_imputations, axis=0), axis=0)\n",
    "    final_avg_imputed_data = force_to_binary(final_avg_imputed_data, df, binary_cols, col_modes)\n",
    "\n",
    "    df_imputed_final = pd.DataFrame(final_avg_imputed_data, columns=df.columns)\n",
    "    df_imputed_final.to_csv(final_output_file, index=False)\n",
    "    print(f\"Saved final averaged imputed CSV to {final_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Simulated Data/simple_gen_kNN4_impute.csv\"\n",
    "\n",
    "# Create and fit the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=4)\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "\n",
    "df_kNN_imputed = pd.DataFrame(imputed_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2fca7b87364476aaed12492d9c9c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Imputing data:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_file = \"Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNN3,5_impute_binary.csv\"\n",
    "\n",
    "# Define the range of k values\n",
    "k_values = [3, 5]\n",
    "imputed_list = []\n",
    "\n",
    "# Loop over each k value, impute the dataset, and store the results\n",
    "for k in tqdm(k_values, desc=\"Imputing data\"):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    imputed_data = imputer.fit_transform(df)\n",
    "    imputed_list.append(imputed_data)\n",
    "\n",
    "# Stack all imputed arrays along a new axis and compute the cell-wise average\n",
    "avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "\n",
    "binary_cols = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"]\n",
    "\n",
    "# Force values to be binary for specified columns\n",
    "for col in binary_cols:\n",
    "    # Compute the majority value (mode) for the current column in the original dataframe\n",
    "    majority_val = df[col].mode().iloc[0]  # In case of ties, mode() returns the first encountered value\n",
    "    col_idx = df.columns.get_loc(col)      # Get the index of the column in the dataframe\n",
    "    col_values = avg_imputed_data[:, col_idx]\n",
    "    \n",
    "    # Create masks for values: less than 0.5, greater than 0.5, and those equal to 0.5 (with tolerance)\n",
    "    mask_lt = col_values < 0.5\n",
    "    mask_gt = col_values > 0.5\n",
    "    mask_eq = np.isclose(col_values, 0.5)\n",
    "    \n",
    "    # Apply threshold rules: <0.5 becomes 0, >0.5 becomes 1, exactly 0.5 becomes the majority value\n",
    "    col_values[mask_lt] = 0\n",
    "    col_values[mask_gt] = 1\n",
    "    col_values[mask_eq] = majority_val\n",
    "    \n",
    "    # Replace the column in the averaged array with the updated values\n",
    "    avg_imputed_data[:, col_idx] = col_values\n",
    "\n",
    "# Create a DataFrame from the averaged imputed data\n",
    "df_kNN_imputed = pd.DataFrame(avg_imputed_data, columns=df.columns)\n",
    "\n",
    "# Save the imputed DataFrame to the output file path\n",
    "df_kNN_imputed.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test kNN n subsamples, only one k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d5bc781821479cae5c1134b585c577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_90090/2438387398.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved averaged imputed dataset for k=2 to Data/Simulated Data/New Simulated Data t=21/kNN/7k 2 to 8 series/simple_gen_kNNavg7k2_impute_binary.csv\n"
     ]
    }
   ],
   "source": [
    "# Number of subsamples and range of k-values\n",
    "n_subsamples = 7\n",
    "k_values = range(2, 3)\n",
    "\n",
    "# Loop over each k value\n",
    "for k in tqdm(k_values):\n",
    "    imputed_list = []  # List to hold the imputed arrays for the current k\n",
    "\n",
    "    # Loop over subsamples (bootstrap replications)\n",
    "    for i in range(n_subsamples):\n",
    "        # Create a stratified bootstrap sample by grouping over 't'\n",
    "        subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
    "        \n",
    "        # Create and fit a KNN imputer with the current k value on the subsample\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        imputer.fit(subsample)\n",
    "        \n",
    "        # Impute the entire dataset using the fitted imputer\n",
    "        imputed_data = imputer.transform(df)\n",
    "        \n",
    "        # Append the imputed result to the list\n",
    "        imputed_list.append(imputed_data)\n",
    "\n",
    "    # Average the imputed arrays across the 10 subsamples for the current k value\n",
    "    avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "\n",
    "    # Force the averaged imputed values to be binary for the specified columns\n",
    "    # (Assumes binary_cols is a list of column names with binary data)\n",
    "    for col in binary_cols:\n",
    "        # Get the majority (mode) value for the column in the original data\n",
    "        majority_val = df[col].mode().iloc[0]\n",
    "        col_idx = df.columns.get_loc(col)\n",
    "        col_values = avg_imputed_data[:, col_idx]\n",
    "\n",
    "        # Create boolean masks for values less than, greater than, and essentially equal to 0.5\n",
    "        mask_lt = col_values < 0.5\n",
    "        mask_gt = col_values > 0.5\n",
    "        mask_eq = np.isclose(col_values, 0.5)\n",
    "\n",
    "        # Apply threshold rules: <0.5 becomes 0, >0.5 becomes 1,\n",
    "        # and values equal to 0.5 are set to the majority value from the original column\n",
    "        col_values[mask_lt] = 0\n",
    "        col_values[mask_gt] = 1\n",
    "        col_values[mask_eq] = majority_val\n",
    "\n",
    "        # Replace the column in the averaged array with the binary values\n",
    "        avg_imputed_data[:, col_idx] = col_values\n",
    "\n",
    "    # Convert the averaged imputed array into a DataFrame (with original column names)\n",
    "    df_avg_imputed = pd.DataFrame(avg_imputed_data, columns=df.columns)\n",
    "\n",
    "    # Build the output filename indicating the current k value\n",
    "    output_file = f\"Data/Simulated Data/New Simulated Data t=21/kNN/{n_subsamples}k 2 to 8 series/simple_gen_kNNavg{n_subsamples}k{k}_impute_binary.csv\"\n",
    "    df_avg_imputed.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print a message to indicate progress\n",
    "    print(f\"Saved averaged imputed dataset for k={k} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_36392/1030654047.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    }
   ],
   "source": [
    "output_file = \"Data/Simulated Data/New Simulated Data t=21/simple_gen_kNNavg10k4_impute.csv\"\n",
    "\n",
    "# Number of subsamples to generate\n",
    "n_subsamples = 10\n",
    "imputed_list = []\n",
    "\n",
    "# Loop to generate n_subsamples bootstrapped (stratified by 't') samples\n",
    "for i in range(n_subsamples):\n",
    "    # For each unique time stamp in 't', sample with replacement\n",
    "    subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
    "    \n",
    "    # Create and fit the KNN imputer on the subsample\n",
    "    imputer = KNNImputer(n_neighbors=4)\n",
    "    imputer.fit(subsample)\n",
    "    \n",
    "    # Use the fitted imputer to impute the entire dataset\n",
    "    imputed_data = imputer.transform(df)\n",
    "    imputed_list.append(imputed_data)\n",
    "\n",
    "# Stack the five imputed arrays along a new axis and compute the cell-wise average\n",
    "avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "\n",
    "# Create a DataFrame from the averaged imputed data\n",
    "df_kNN_imputed = pd.DataFrame(avg_imputed_data, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg kNN across subsamples + multiple k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO REWRITE: Have it impute for each subsample across multiple k, then save each of those imputed datasets (individually, not stacked with previous). Then edit code afterwards to apply averaging or majority vote method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 1 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg1k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 2 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg2k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 3 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg3k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 4 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg4k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 5 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg5k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 6 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg6k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 7 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg7k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 8 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg8k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 9 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg9k26_impute_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_23543/1226389135.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial dataset after subsample 10 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg10k26_impute_new.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = \"Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg10k26_impute_new.csv\"\n",
    "\n",
    "# Number of subsamples and range of k-values\n",
    "n_subsamples = 3\n",
    "k_values = range(2, 7)\n",
    "\n",
    "imputed_list = []\n",
    "\n",
    "# Outer loop: iterate over subsamples (stratified bootstrap by 't')\n",
    "for i in range(n_subsamples):\n",
    "    # For each unique time stamp in 't', sample with replacement\n",
    "    subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
    "    \n",
    "    # Inner loop: iterate over each k-value\n",
    "    for k in k_values:\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        # Fit the imputer on the subsample\n",
    "        imputer.fit(subsample)\n",
    "        # Use the fitted imputer to impute the entire dataset\n",
    "        imputed_data = imputer.transform(df)\n",
    "        \n",
    "        # Append this imputed array to our growing list\n",
    "        imputed_list.append(imputed_data)\n",
    "    \n",
    "    # After finishing all k values in a subsample, compute the average of all imputations so far\n",
    "    partial_avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "    df_partial = pd.DataFrame(partial_avg_imputed_data, columns=df.columns)\n",
    "    \n",
    "    # Save this partial dataset to a file\n",
    "    partial_output_file = f\"Data/Simulated Data/New Simulated Data t=21/kNN/Nonbinary/simple_gen_kNNavg{i+1}k26_impute_new.csv\"\n",
    "    df_partial.to_csv(partial_output_file, index=False)\n",
    "    print(f\"Saved partial dataset after subsample {i+1} to {partial_output_file}\")\n",
    "\n",
    "# Average across all subsamples\n",
    "avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "df_kNN_imputed = pd.DataFrame(avg_imputed_data, columns=df.columns)\n",
    "\n",
    "# Write out the final result\n",
    "df_kNN_imputed.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate binary results (loop across subsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved binary threshold imputed dataset for subsample 1 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg1k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 2 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg2k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 3 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg3k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 4 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg4k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 5 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg5k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 6 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg6k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 7 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg7k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 8 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg8k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 9 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg9k26_impute_binary.csv\n",
      "Saved binary threshold imputed dataset for subsample 10 to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg10k26_impute_binary.csv\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Average imputed values, then force to binary based on 0.5 threshold\n",
    "\n",
    "binary_cols = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\"]\n",
    "\n",
    "# Compute the prevalence (fraction of ones) for each binary column in the original data\n",
    "col_prevalences = {}\n",
    "for col in binary_cols:\n",
    "    valid_values = df[col].dropna()\n",
    "    if len(valid_values) == 0:\n",
    "        col_prevalences[col] = 0.5  # Default prevalence if the column is completely missing\n",
    "    else:\n",
    "        col_prevalences[col] = valid_values.mean()\n",
    "\n",
    "# Loop over subsample indices 1 to 10 and process each CSV file\n",
    "for i in range(1, 11):\n",
    "    # Construct the input file path for the nonbinary imputed CSV\n",
    "    input_file = f\"Data/Simulated Data/New Simulated Data t=21/kNN/Nonbinary/simple_gen_kNNavg{i}k26_impute_new.csv\"\n",
    "    # Read the imputed CSV (nonbinary version)\n",
    "    df_kNN_imputed = pd.read_csv(input_file)\n",
    "    \n",
    "    # Apply thresholding to force binary values on the binary columns\n",
    "    for col in binary_cols:\n",
    "        # Identify rows where the imputed value is exactly 0.5 (tie)\n",
    "        is_half = df_kNN_imputed[col] == 0.5\n",
    "        \n",
    "        # Force values: below 0.5 become 0, above 0.5 become 1\n",
    "        df_kNN_imputed.loc[df_kNN_imputed[col] < 0.5, col] = 0\n",
    "        df_kNN_imputed.loc[df_kNN_imputed[col] > 0.5, col] = 1\n",
    "        \n",
    "        # For values equal to 0.5, break tie based on the original column prevalence\n",
    "        if col_prevalences[col] >= 0.5:\n",
    "            df_kNN_imputed.loc[is_half, col] = 1\n",
    "        else:\n",
    "            df_kNN_imputed.loc[is_half, col] = 0\n",
    "        \n",
    "        # Ensure the column is of integer type\n",
    "        df_kNN_imputed[col] = df_kNN_imputed[col].astype(int)\n",
    "    \n",
    "    # Define the output file path for the binary (thresholded) dataset.\n",
    "    output_file = f\"Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNavg{i}k26_impute_binary.csv\"\n",
    "    # Save the processed DataFrame to CSV\n",
    "    df_kNN_imputed.to_csv(output_file, index=False)\n",
    "    print(f\"Saved binary threshold imputed dataset for subsample {i} to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved majority vote imputed dataset using 1 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj1k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 2 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj2k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 3 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj3k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 4 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj4k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 5 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj5k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 6 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj6k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 7 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj7k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 8 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj8k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 9 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj9k26_impute_majority.csv\n",
      "Saved majority vote imputed dataset using 10 subsample(s) to Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj10k26_impute_majority.csv\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Force all imputed subsamples to binary, then do majority vote per cell\n",
    "\n",
    "binary_cols = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\"]\n",
    "\n",
    "# Compute the prevalence (fraction of ones) for each binary column in the original data\n",
    "col_prevalences = {}\n",
    "for col in binary_cols:\n",
    "    valid_values = df[col].dropna()\n",
    "    if len(valid_values) == 0:\n",
    "        col_prevalences[col] = 0.5  # Default prevalence if the column is completely missing\n",
    "    else:\n",
    "        col_prevalences[col] = valid_values.mean()\n",
    "\n",
    "# Each subsample produces 5 imputations (since len(k_values)==5)\n",
    "k_per_subsample = len(k_values)\n",
    "\n",
    "# Loop over the number of subsamples (1 to n_subsamples)\n",
    "for i in range(1, n_subsamples + 1):\n",
    "    # For i subsamples, take the first i*5 imputations from imputed_list\n",
    "    num_imputations = i * k_per_subsample\n",
    "    subset_imputed_list = imputed_list[:num_imputations]\n",
    "    \n",
    "    # Stack the selected imputations into a 3D array:\n",
    "    # Shape: (num_imputations, n_rows, n_columns)\n",
    "    imputed_array_subset = np.stack(subset_imputed_list, axis=0)\n",
    "    \n",
    "    # Start with the average imputed data for non-binary columns\n",
    "    df_majority_partial = pd.DataFrame(np.mean(imputed_array_subset, axis=0), columns=df.columns)\n",
    "    \n",
    "    # Apply majority vote for each binary column:\n",
    "    for col in binary_cols:\n",
    "        # Get the column index in the dataframe\n",
    "        col_idx = df.columns.get_loc(col)\n",
    "        # Extract imputed values for this column across all selected imputations\n",
    "        col_imputations = imputed_array_subset[:, :, col_idx]\n",
    "        \n",
    "        # Convert continuous imputed values to binary votes:\n",
    "        # Values < 0.5 become 0, values > 0.5 become 1,\n",
    "        # and values exactly 0.5 are set to np.nan (to mark ties).\n",
    "        binary_imputations = np.where(col_imputations < 0.5, 0,\n",
    "                                      np.where(col_imputations > 0.5, 1, np.nan))\n",
    "        \n",
    "        # For tie cases (np.nan), assign based on the original prevalence:\n",
    "        tie_mask = np.isnan(binary_imputations)\n",
    "        binary_imputations[tie_mask] = 1 if col_prevalences[col] >= 0.5 else 0\n",
    "        \n",
    "        # Compute the mode (majority vote) across the imputations axis (axis=0) for each row.\n",
    "        maj_vote, _ = mode(binary_imputations, axis=0, keepdims=False)\n",
    "        # Update the binary column in the DataFrame with the majority vote, cast to int.\n",
    "        df_majority_partial[col] = maj_vote.astype(int)\n",
    "    \n",
    "    # Save the resulting majority voted imputed dataset for i subsamples.\n",
    "    output_file_majority = f\"Data/Simulated Data/New Simulated Data t=21/kNN/simple_gen_kNNmaj{i}k26_impute_binary.csv\"\n",
    "    df_majority_partial.to_csv(output_file_majority, index=False)\n",
    "    print(f\"Saved majority vote imputed dataset using {i} subsample(s) to {output_file_majority}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS (faster kNN-like algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_knn_impute_ref(query, reference, k):\n",
    "    n_query, n_features = query.shape\n",
    "    n_ref = reference.shape[0]\n",
    "    \n",
    "    # Compute the column modes from the reference (ignoring NaNs)\n",
    "    col_modes = np.empty(n_features)\n",
    "    for j in range(n_features):\n",
    "        valid_vals = reference[:, j][~np.isnan(reference[:, j])]\n",
    "        if len(valid_vals) == 0:\n",
    "            col_modes[j] = 0  # default if entirely missing\n",
    "        else:\n",
    "            # Ensure the result is always an array before indexing:\n",
    "            col_modes[j] = np.atleast_1d(mode(valid_vals).mode)[0]\n",
    "    \n",
    "    # Fill missing values in reference with column modes for indexing\n",
    "    reference_filled = np.where(np.isnan(reference), np.tile(col_modes, (n_ref, 1)), reference)\n",
    "    \n",
    "    # Build FAISS index (using L2 distance)\n",
    "    d = n_features\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    reference_filled = np.ascontiguousarray(reference_filled.astype(np.float32))\n",
    "    index.add(reference_filled)\n",
    "    \n",
    "    # Create a filled version of query for initial imputation (using column modes)\n",
    "    query_filled = np.where(np.isnan(query), np.tile(col_modes, (n_query, 1)), query)\n",
    "    X_imputed = query_filled.copy()\n",
    "    \n",
    "    # For each query row with missing values, impute missing features using weighted average\n",
    "    for i in range(n_query):\n",
    "        missing = np.isnan(query[i, :])\n",
    "        if not np.any(missing):\n",
    "            continue  # Skip rows with no missing values\n",
    "        q_row = np.ascontiguousarray(query_filled[i:i+1].astype(np.float32))\n",
    "        D, I = index.search(q_row, k + 1)\n",
    "        if D[0, 0] == 0:\n",
    "            neighbors = I[0][1:k+1]\n",
    "            distances = D[0][1:k+1]\n",
    "        else:\n",
    "            neighbors = I[0][:k]\n",
    "            distances = D[0][:k]\n",
    "        for j in np.where(missing)[0]:\n",
    "            neighbor_vals = []\n",
    "            neighbor_weights = []\n",
    "            epsilon = 1e-6\n",
    "            for n, d in zip(neighbors, distances):\n",
    "                if not np.isnan(reference[n, j]):\n",
    "                    neighbor_vals.append(reference[n, j])\n",
    "                    neighbor_weights.append(1.0 / (d + epsilon))\n",
    "            if len(neighbor_vals) > 0:\n",
    "                X_imputed[i, j] = np.average(neighbor_vals, weights=neighbor_weights)\n",
    "            else:\n",
    "                X_imputed[i, j] = col_modes[j]\n",
    "    return X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad29537401f946b0a3d6c17b8e1dfff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing subsamples:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab1b04ed64a460498859e9dbc83dc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 1 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample1_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d377c03c383244ecae5fa566a2d5822d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 2 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample2_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdf4131145f4410b71da411c213e240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 3 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample3_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22954264fa2d48b3add64abb862c5052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 4 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample4_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c864dc69e1943958efb4382011e9e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 5 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample5_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e97aaaf2bb4d5781cccc736c7b6c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 6 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample6_v3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/z0msnbbj10vdbs6czbvpsmy40000gn/T/ipykernel_76424/1180036936.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668355914d884818b44f372dd4726472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing k-values:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 7 imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample7_v3.csv\n",
      "Saved overall averaged imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_v3.csv\n"
     ]
    }
   ],
   "source": [
    "# ----- MAIN IMPUTATION CODE -----\n",
    "output_file = \"Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_v3.csv\"\n",
    "\n",
    "# Number of subsamples and range of k-values\n",
    "n_subsamples = 7\n",
    "k_values = range(2, 7)\n",
    "\n",
    "imputed_list = []\n",
    "\n",
    "# Outer loop: iterate over subsamples (stratified bootstrap by 't') with a progress bar\n",
    "for i in tqdm(range(n_subsamples), desc=\"Processing subsamples\"):\n",
    "    # For each unique time stamp in 't', sample with replacement\n",
    "    subsample = df.groupby('t', group_keys=False).apply(lambda x: x.sample(frac=1, replace=True))\n",
    "    \n",
    "    # List to hold imputed data for each k within the current subsample\n",
    "    subsample_imputations = []\n",
    "    \n",
    "    # Inner loop: iterate over each k-value with a progress bar\n",
    "    for k in tqdm(k_values, desc=\"Processing k-values\", leave=False):\n",
    "        # Use FAISS-based kNN imputation (using the subsample as reference to impute df)\n",
    "        imputed_data = faiss_knn_impute_ref(df.to_numpy(), subsample.to_numpy(), k)\n",
    "        subsample_imputations.append(imputed_data)\n",
    "    \n",
    "    # Compute the average imputation for the current subsample (averaging over k-values)\n",
    "    sample_avg_imputed_data = np.mean(np.stack(subsample_imputations, axis=0), axis=0)\n",
    "    \n",
    "    # Save this sample's imputed data as a CSV file (without combining with previous samples)\n",
    "    sample_output_file = f\"Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_sample{i+1}_v3.csv\"\n",
    "    df_sample = pd.DataFrame(sample_avg_imputed_data, columns=df.columns)\n",
    "    df_sample.to_csv(sample_output_file, index=False)\n",
    "    print(f\"Saved sample {i+1} imputed dataset to {sample_output_file}\")\n",
    "    \n",
    "    # Append this sample's imputed data to our list\n",
    "    imputed_list.append(sample_avg_imputed_data)\n",
    "\n",
    "# After processing all subsamples, average across all samples (cell-wise average)\n",
    "avg_imputed_data = np.mean(np.stack(imputed_list, axis=0), axis=0)\n",
    "df_kNN_imputed = pd.DataFrame(avg_imputed_data, columns=df.columns)\n",
    "\n",
    "# Write out the final averaged imputed result\n",
    "df_kNN_imputed.to_csv(output_file, index=False)\n",
    "print(f\"Saved overall averaged imputed dataset to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved binary threshold imputed dataset to Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_v3_binary.csv\n"
     ]
    }
   ],
   "source": [
    "# ----- BINARY THRESHOLDING -----\n",
    "final_input_file = \"Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_v3.csv\"\n",
    "final_output_file = \"Data/Simulated Data/New Simulated Data t=21/kNN/FAISS 7k26/simple_gen_FAISSavg7k26_impute_v3_binary.csv\"\n",
    "\n",
    "# Read the final averaged imputed dataset\n",
    "df_kNN_imputed = pd.read_csv(final_input_file)\n",
    "\n",
    "# Define the binary columns (adjust these as needed)\n",
    "binary_cols = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"]\n",
    "\n",
    "# Compute the prevalence (fraction of ones) for each binary column in the original data\n",
    "col_prevalences = {}\n",
    "for col in binary_cols:\n",
    "    valid_values = df[col].dropna()  # using original df for prevalence\n",
    "    if len(valid_values) == 0:\n",
    "        col_prevalences[col] = 0.5  # Default if completely missing\n",
    "    else:\n",
    "        col_prevalences[col] = valid_values.mean()\n",
    "\n",
    "# Apply thresholding: for each binary column, set values <0.5 to 0, >0.5 to 1, and for ties (==0.5) break based on prevalence\n",
    "for col in binary_cols:\n",
    "    is_half = df_kNN_imputed[col] == 0.5\n",
    "    df_kNN_imputed.loc[df_kNN_imputed[col] < 0.5, col] = 0\n",
    "    df_kNN_imputed.loc[df_kNN_imputed[col] > 0.5, col] = 1\n",
    "    df_kNN_imputed.loc[is_half, col] = 1 if col_prevalences[col] >= 0.5 else 0\n",
    "    df_kNN_imputed[col] = df_kNN_imputed[col].astype(int)\n",
    "\n",
    "# Save the binary thresholded dataset\n",
    "df_kNN_imputed.to_csv(final_output_file, index=False)\n",
    "print(f\"Saved binary threshold imputed dataset to {final_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-looping k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average imputed values, then force to binary based on 0.5 threshold\n",
    "df_kNN_imputed = pd.read_csv('')\n",
    "\n",
    "binary_cols = [\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\"]\n",
    "\n",
    "col_prevalences = {}\n",
    "for col in binary_cols:\n",
    "    valid_values = df[col].dropna()  # original data, ignoring missing\n",
    "    if len(valid_values) == 0:\n",
    "        # If an entire column was missing, we default its prevalence to 0.5\n",
    "        col_prevalences[col] = 0.5\n",
    "    else:\n",
    "        # Mean of binary values = fraction of ones\n",
    "        col_prevalences[col] = valid_values.mean()\n",
    "\n",
    "# Threshold only the binary columns\n",
    "for col in binary_cols:\n",
    "    is_half = df_kNN_imputed[col] == 0.5\n",
    "    \n",
    "    # Values < 0.5 -> 0\n",
    "    df_kNN_imputed.loc[df_kNN_imputed[col] < 0.5, col] = 0\n",
    "    \n",
    "    # Values > 0.5 -> 1\n",
    "    df_kNN_imputed.loc[df_kNN_imputed[col] > 0.5, col] = 1\n",
    "    \n",
    "    # Values == 0.5 -> break tie based on prevalence in original data\n",
    "    if col_prevalences[col] >= 0.5:\n",
    "        df_kNN_imputed.loc[is_half, col] = 1\n",
    "    else:\n",
    "        df_kNN_imputed.loc[is_half, col] = 0\n",
    "    \n",
    "    # Convert to integer\n",
    "    df_kNN_imputed[col] = df_kNN_imputed[col].astype(int)\n",
    "\n",
    "df_kNN_imputed.to_csv(\"\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treeple Unsupervised Oblique Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Data/Simulated Data/New Simulated Data t=21/simple_gen_geodesic_v1.csv\"\n",
    "\n",
    "X = df[vars].values\n",
    "\n",
    "# Pre-impute missing values using the \"mode\" strategy\n",
    "pre_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "X_pre_imputed = pre_imputer.fit_transform(X)\n",
    "\n",
    "# Create the imputer using treeple's implementation on the pre-imputed data\n",
    "imputer = UnsupervisedRandomForest(n_estimators=10, max_depth=3, \n",
    "                                          min_samples_leaf=1, random_state=42)\n",
    "X_imputed = imputer.fit_transform(X_pre_imputed)\n",
    "\n",
    "# Convert back to a DataFrame\n",
    "df_imputed = pd.DataFrame(X_imputed, columns=vars)\n",
    "\n",
    "# Save the imputed dataset\n",
    "df_imputed.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
